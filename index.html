<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.16" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Human Learning Machine Learning &middot; Human Learning Machine Learning </title>

  
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/poole.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/syntax.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://gavinln.github.io/blog-site/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="http://gavinln.github.io/blog-site/favicon.png">

  
  <link href="http://gavinln.github.io/blog-site/index.xml" rel="alternate" type="application/rss+xml" title="Human Learning Machine Learning" />
</head>

<body class="theme-base-0b">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://gavinln.github.io/blog-site/"><h1>Human Learning Machine Learning</h1></a>
      <p class="lead">
         
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://gavinln.github.io/blog-site/">Home</a> </li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/"> Accuracy, precision, recall for machine learning </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/cross_validation/"> Cross validation </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/scaling-input-data/"> Scaling input data </a></li>
      
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scaling-input-data/">
        Scaling input data
      </a>
    </h1>

    <span class="post-date">Wed, Sep 21, 2016</span>

    <p>Some machine learning algorithms in the sklearn library are affected by data measured using different scales. For example if one measure is in centimeters (one hundreth of a meter) and another is in micrometers (one millionth of a meter) the results may not be optimal.</p>

<p>Sklearn has scaling functions in the <a href="http://scikit-learn.org/stable/modules/preprocessing.html">preprocessing</a> module</p>

<p>Load the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html">Iris</a> dataset. It has three kinds of Irises with each Iris species stored in 50 consecutive rows. Loading the second and third Iris species stored from row 50 and above create a subset of the data.</p>

<p>Display the two species <em>versicolor</em> and <em>virginica</em> with two axis: sepal length and sepal width. The sepal length is in centimeters and the sepal width is in micrometers.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/scaling_input_data/iris-species-different-scales.png" width="600" />
    
    
    <figcaption>
        <h4>Iris species: different scales on x and y axes</h4>
        
    </figcaption>
    
</figure>


<p>Using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html">scale</a> function in sklearn the measures are converted to have a mean of zero and variance of one.</p>

<table>
<thead>
<tr>
<th>measure</th>
<th>sepal length</th>
<th>sepal width</th>
<th>species</th>
</tr>
</thead>

<tbody>
<tr>
<td>count</td>
<td>1.000000e+02</td>
<td>1.000000e+02</td>
<td>100.0000</td>
</tr>

<tr>
<td>mean</td>
<td>3.288481e-15</td>
<td>4.496403e-17</td>
<td>1.500000</td>
</tr>

<tr>
<td>stdev</td>
<td>1.005038e+00</td>
<td>1.005038e+00</td>
<td>0.502519</td>
</tr>
</tbody>
</table>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/scaling_input_data/iris-species-normalized-axes.png" width="600" />
    
    
    <figcaption>
        <h4>Iris species: normalized axes</h4>
        
    </figcaption>
    
</figure>


<p>Using logistic regression and a decision tree classifier with the original data
and normalized data the results are in the table below.</p>

<table>
<thead>
<tr>
<th>classifier</th>
<th>accuracy</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>

<tbody>
<tr>
<td>Logistic regression: different scales</td>
<td>0.56</td>
<td>0.578947</td>
<td>0.44</td>
</tr>

<tr>
<td>Logistic regression: normalized data</td>
<td>0.75</td>
<td>0.745098</td>
<td>0.76</td>
</tr>

<tr>
<td>Decision tree: different scales</td>
<td>0.88</td>
<td>0.816667</td>
<td>0.98</td>
</tr>

<tr>
<td>Decision tree: normalized data</td>
<td>0.88</td>
<td>0.816667</td>
<td>0.98</td>
</tr>
</tbody>
</table>

<p>The logistic regression method improves when using normalized data however the
decision tree method does equally well with the original data and the
normalized data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/cross_validation/">
        Cross validation
      </a>
    </h1>

    <span class="post-date">Thu, Apr 14, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">Cross-validation</a> is a technique used to assess how a statistical analysis will generalize to an independent data set.</p>

<p>When creating a predictive model, the model is trained using a dataset called the training dataset. The accuracy of the trained model is then tested on another unknown dataset called the testing dataset. The process is called cross-validation.</p>

<p><a href="http://scikit-learn.org/stable/modules/cross_validation.html">Scikit</a> learn makes it easy to use multiple methods for cross validation. A basic approach is called k-fold cross validation. The dataset is split into k smaller sets, where 1 of the sets is used to validate the model while the remaining are used to train the model. The <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">peformance measures</a> reported by the k-fold cross-validations are the average of the values computed by choosing a different set for the cross-validation and using the remaining for training.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/08_Cross_validation.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The &ldquo;Wisconsin Breast Cancer&rdquo; dataset is used to demonstrate cross-validation. This data set has 569 samples of which 357 are benign and 212 are malignant. Ten factors are used to predict breast cancer.</p>

<p>In addition to precision and recall, the <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> is calculated. The F1 score is the harmonic mean and equally weights precision and recall. A F1 score reaches its highest value at 1 and lowest value at 0.</p>

<p>The four classifiers: logistic regression, support vector, decision tree and random forests are compared on the cross-validation scores. They perform much worse on the test dataset as compared to the training dataset. Compare the results with those in the <a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/">previous post</a>.</p>

<table>
<thead>
<tr>
<th>classifier</th>
<th>accuracy</th>
<th>precision</th>
<th>recall</th>
<th>f1_score</th>
</tr>
</thead>

<tbody>
<tr>
<td>logistic regression</td>
<td>0.926186</td>
<td>0.938719</td>
<td>0.943978</td>
<td>0.941341</td>
</tr>

<tr>
<td>support vector (radial basis)</td>
<td>0.717047</td>
<td>0.704167</td>
<td>0.946779</td>
<td>0.807646</td>
</tr>

<tr>
<td>decision tree</td>
<td>0.905097</td>
<td>0.922006</td>
<td>0.927171</td>
<td>0.924581</td>
</tr>

<tr>
<td>random forest</td>
<td>0.947276</td>
<td>0.955432</td>
<td>0.960784</td>
<td>0.958101</td>
</tr>
</tbody>
</table>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/">
        Accuracy, precision, recall for machine learning
      </a>
    </h1>

    <span class="post-date">Sun, Apr 10, 2016</span>

    <p>A popular way to evaluate the performance of a machine learning algorithm is to use a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>. This is a table with two rows and two columns that displays the number of true positives, false positives, false negatives and true negatives.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/07_Precision_and_Recall.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The table below shows an example confusion matrix for a hypothetical test for a rare disease where only 2 people of out 100 have the disease. This is an unbalanced data set as a much larger number, 98 out of 100 do not have the disease. The first named row has cases of people who have the disease and the second named row has cases of people who do not have the disease. The first named column has people who test positive and the second named column has people who test negative.</p>

<p>This leads to four numeric cell with the top left containing true positive counts, the bottom left having false positive, the top right having false negative and the bottom right with true negative counts.</p>

<p>A simple way to create a very accurate test for this unbalanced example is to just assume everyone tests negative for the disease. This misses out on all the people who do actually have the disease and results in two false negative cases. However it correctly predicts 98 true negative cases. This results in a 98% accurate test. But this test cannot distinguish between people who have a disease and people who don&rsquo;t. Accuracy may not be a useful measure of the goodness of the test.</p>

<p>Two useful measures are precision and recall: Precision is a measure of how many of the selected items are relevant and recall is a measure of how many relevant items are selected.</p>

<p>precision = (true positives)/(true positives + false positives)</p>

<p>recall = (True positives)/positives</p>

<p>In the example below the precision is undefined while the recall is zero.</p>

<table>
<thead>
<tr>
<th></th>
<th align="right">predicted condition positive</th>
<th align="right">predicted condition negative</th>
</tr>
</thead>

<tbody>
<tr>
<td>true condition positive</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>

<tr>
<td>true condition negative</td>
<td align="right">0</td>
<td align="right">98</td>
</tr>
</tbody>
</table>

<p>An alternative test for the same rare disease where 2 out of 100 have the disease is show below. Now there is 1 true positive, 2 false positives, 1 false negative and 96 true negatives.</p>

<p>This test has a lower accuracy as it has correct predicted 97 out of 100 cases, lower than the previous test. This test also has a defined precision of 0.333333 and a recall of 0.5</p>

<p>This test correctly identifies 1 out of the 2 people who have the disease.</p>

<table>
<thead>
<tr>
<th></th>
<th align="right">predicted condition positive</th>
<th align="right">predicted condition negative</th>
</tr>
</thead>

<tbody>
<tr>
<td>true condition positive</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>

<tr>
<td>true condition negative</td>
<td align="right">2</td>
<td align="right">96</td>
</tr>
</tbody>
</table>

<p>To demonstrate the use of accuracy, precision and recall when measuring the peformance of a classifier, we use the &ldquo;Wisconsin Breast Cancer&rdquo; data set.</p>

<p>This data set has 569 samples of which 357 are benign and 212 are malignant</p>

<p>We predict whether the cancer is benign or malignant using ten factors: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and fractal dimension.</p>

<p>We compare four classifiers: logistic regression, support vector, decision tree and random forests on three different measures, accuracy, precision and recall. The decision tree and random forest classifiers are so good that they correctly classify 100% of the samples in this data set.</p>

<table>
<thead>
<tr>
<th>classifier</th>
<th align="right">accuracy</th>
<th align="right">precision</th>
<th align="right">recall</th>
</tr>
</thead>

<tbody>
<tr>
<td>logistic regression</td>
<td align="right">0.929701</td>
<td align="right">0.927224</td>
<td align="right">0.963585</td>
</tr>

<tr>
<td>support vector (radial basis)</td>
<td align="right">0.987698</td>
<td align="right">0.980769</td>
<td align="right">1</td>
</tr>

<tr>
<td>decision tree</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>

<tr>
<td>random forest</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>However the performance of a classifier on the training data is not necessarily
a good indication of how well it will do on data it has not seen. In future
posts the classifiers will be tested on new data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/k_nearest_neighbors/">
        K nearest neighbors using Scikit-learn
      </a>
    </h1>

    <span class="post-date">Sun, Mar 27, 2016</span>

    <p>The <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nearest neighbors</a> algorithm is one of the simplest algorithms for machine learning. It is a non-parametric method used for both classification and regression.</p>

<p>In a classification problem an object is classified by a majority vote of its neighbors. Typically k is a small positive integer. If k = 1, the object is assigned to be the class of the nearest neighbor. If k = 3 the object is assigned to be in the class of the nearest 2 neighbors and so on for different values of k.</p>

<p>In a regression problem, the property of the object is assigned a value that is the average of the values of its k nearest neighbors.</p>

<p>The <a href="http://scikit-learn.org/stable/modules/neighbors.html">Scikit-learn</a> library module KNeighborsClassifier demonstrates the use
of the k-nearest neighbor algorithm for classification.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/06_K_nearest_neighbors.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &ldquo;0&rdquo;, &ldquo;1&rdquo;, &ldquo;2&rdquo; (setosa, versicolor, virginica).</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The K-nearest neighbors classifier is used to predict the species by using just two features: &ldquo;sepal length&rdquo; and &ldquo;petal width&rdquo;.</p>

<p>The graphs below show the predictions of the k-nearest neighbors algorithm using three different values for the number of nearest neighbors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/k_nearest_neighbors/iris_multiple_values_k_nearest_neighbors.png" width="600" />
    
    
    <figcaption>
        <h4>Using k-nearest neighbors to predict Iris species</h4>
        
    </figcaption>
    
</figure>


<p>When the k value is small (like the graph on the left) the decision boundary is relatively complex and even though the algorithm predicts the training data well, it is likely over-fitting the data and fair poorly on a new sample. For a very high value of k (like the graph on the right) the method the decision boundary is simpler and likely to under-fit the training data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scikit-random-forests/">
        Random forests using Scikit-learn
      </a>
    </h1>

    <span class="post-date">Sun, Mar 20, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Random_forest">Random forests</a> is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning</a> method. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained for any of the constituent learning algorithms.</p>

<p>Random forests work by constructing multiple decision trees and combining the trees. The algorithm was developed by <a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman</a> and Adele Cutler and &ldquo;Random Forests&rdquo; is their trademark.</p>

<p>Random forests correct for decision trees&rsquo; habit of over-fitting to their training data set.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/05_Random_Forests.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>To demonstrate the tendency of decision trees to overfit the data we predict the species of Iris using just two features: sepal length and petal width. The species are shown in a scatter plot in different colors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The graphs below show three Iris species using three different colors and the shaded regions predicted by the decision tree using lighter shades of the same colors. Each of the three plots in the set uses a different random sample made up of 70% of the data set. The decision tree boundaries are different in each case. This is an indication of over-fitting.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/random_forests/decision_trees-iris-multiple_subsets.png" width="600" />
    
    
    <figcaption>
        <h4>Using decision trees to predict Iris species</h4>
        
    </figcaption>
    
</figure>


<p>A similar plot shows a Random Forest Classifier with 500 trees each time used to select various sub-samples of the dataset. This controls over-fitting.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/random_forests/random_forests-iris-multiple_subsets.png" width="600" />
    
    
    <figcaption>
        <h4>Using Random Forests to predict Iris species</h4>
        
    </figcaption>
    
</figure>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/decision-trees-scikit/">
        Decision trees on the Iris data set
      </a>
    </h1>

    <span class="post-date">Sun, Mar 13, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Decision_tree">Decision trees</a> are a non-parametric learning method used for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a>. Trees are often represented with a graph like model where each note is a test and each branch represents the outcome of the test.</p>

<p>We use the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a> to demonstrate the use of a decision tree classifier.</p>

<p>The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &ldquo;0&rdquo;, &ldquo;1&rdquo;, &ldquo;2&rdquo; (setosa, versicolor, virginica).</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/04_Decision_trees.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>To better display the performance of the decision trees algorithm we predict
the species of Iris using just two features: petal length and petal width. The
species are shown in a scatter plot in different colors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-petal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The output of the decision tree is shown using shaded regions that match the colors used to identify the flower. Using a decision tree with various depths the three species of Iris are classified, ineffectively at first with a tree of only one layer. As the number of layers increase the decision tree does a better job identifying the Iris species.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-multiple_depths.png" width="800" />
    
    
    <figcaption>
        <h4>Decision trees classification boundaries</h4>
        
    </figcaption>
    
</figure>


<p>The decision tree rules can also be represented using a graph like drawing with the root node on the left and the leaf nodes on the right.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-1.png" width="400" />
    
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-2.png" width="600" />
    
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-3.png" width="800" />
    
    
</figure>
</p>

<p>Finally we use a decision tree without limiting the depth. It classifies all the flowers correctly.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-unlimited_depth.png" width="600" />
    
    
</figure>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scikit-svm/">
        Support vector machines with Scikit learn
      </a>
    </h1>

    <span class="post-date">Sun, Mar 6, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support vector machines</a> are supervised learning models used for
classification and regression. For a classifier the data is represented as
points in space and a SVM classifier (SVC) separates the classes by a gap that
is as wide as possible. SVM algorithms are known as maximum margin classifiers.</p>

<p>To illustrate the SVC algorithm we generate random points in two dimensions
arranged in two clusters. This is illustrated in a Jupyter (IPython) notebook
in this <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/03_Support_vector_machines.ipynb">repository</a>.</p>


X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
from sklearn.svm import SVC
clf = SVC(kernel='linear')
clf.fit(X, y)



<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/linear_boundaries_two_clusters.png" width="500" />
    
    
    <figcaption>
        <h4>Two lines separating two clusters</h4>
        
    </figcaption>
    
</figure>


<p>Multiple lines can be drawn to separate the clusters. The black line is
preferred to the red line as there is a larger margin between it and the
nearest points.</p>

<p>Some of the points nearest the boundary are known as support vectors. They
margins and the support vectors are plotted below.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM_decision_function-margin-support_vectors.png" width="500" />
    
    
    <figcaption>
        <h4>Support vectors</h4>
        
    </figcaption>
    
</figure>


<p>Support vector classifiers are linear classifiers. For datasets that are not
linearly separable they do a poor job.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM-non_linearly_separable_data.png" width="500" />
    
    
    <figcaption>
        <h4>Non-linearly separable data</h4>
        
    </figcaption>
    
</figure>


<p>To create non-linear boundaries we could convert this two dimensional data set
to higher dimensions. For example we could add the distance of the points from
the origin as the third dimension. The two clusters will then be easily
separable.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM-with_radial_basis_functions.png" width="500" />
    
    
    <figcaption>
        <h4>Radial basis functions for SVC</h4>
        
    </figcaption>
    
</figure>


<p>Another example with non-linearly separable data.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/radial_basis_functions-non_linearly_separable_data.png" width="500" />
    
    
    <figcaption>
        <h4>Radial basis functions for SVC</h4>
        
    </figcaption>
    
</figure>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scikit-logistic_reg-iris/">
        Logistic regression on the Iris data set
      </a>
    </h1>

    <span class="post-date">Mon, Feb 29, 2016</span>

    <p>The <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a> has four features for Iris flower.</p>

<ul>
<li>sepal length</li>
<li>sepal width</li>
<li>petal length</li>
<li>petal width</li>
</ul>

<p>Using a three class <a href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html">logistic regression</a> the four features can be used to
classify the flowers into three species (Iris setosa, Iris virginica,
Iris versicolor).</p>

<p>Using this Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/02_Iris_dataset_logistic_regression.ipynb">notebook</a> combinations of two features we are used to
classify the species. The mis-predicted values are shown below.</p>

<table>
<thead>
<tr>
<th>measure 1</th>
<th>measure 2</th>
<th>incorrect predictions</th>
</tr>
</thead>

<tbody>
<tr>
<td>sepal length</td>
<td>sepal width</td>
<td>29</td>
</tr>

<tr>
<td>sepal length</td>
<td>petal length</td>
<td>6</td>
</tr>

<tr>
<td>sepal length</td>
<td>petal width</td>
<td>8</td>
</tr>

<tr>
<td>sepal width</td>
<td>petal length</td>
<td>7</td>
</tr>

<tr>
<td>sepal width</td>
<td>petal width</td>
<td>7</td>
</tr>

<tr>
<td>petal length</td>
<td>petal width</td>
<td>6</td>
</tr>
</tbody>
</table>

<p>The previous <a href="http://gavinln.github.io/blog-site/post/scikit-pca-iris/">post</a> shows that some combinations of
features are easier to use to separate the species than others.</p>

<p>Logistic regression can also be used on the two principal components and
mis-predicts five specimens.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/seaborn-iris-two-principal-components-mis-predicted.png" width="800" />
    
    
    <figcaption>
        <h4>Iris plot with mis-predicted items</h4>
        
    </figcaption>
    
</figure>


<p>A mesh when drawn over the plot shows the three classes of the logistic
regression.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/seaborn-iris-principal-components-logistic-reg-mesh.png" width="800" />
    
    
    <figcaption>
        <h4>Iris plot - logistic regression</h4>
        
    </figcaption>
    
</figure>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scikit-pca-iris/">
        PCA with Scikit learn on the Iris data set
      </a>
    </h1>

    <span class="post-date">Tue, Feb 23, 2016</span>

    <p><a href="http://scikit-learn.org/">Scikit learn</a> has multiple data sets included with the library. One of the most
well known data sets is the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a> introduced by Ronald Fisher.</p>

<p>Four features were measured from each sample: the length and the width of the
sepals and petals, in centimetres. Sepals are usually green and typically
function as protection for the flower in bud, and often as support for the
petals when in bloom. Based on the combination of these four features the
goal is to distinguish between three species of Iris
(Iris setosa, Iris virginica and Iris versicolor).</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_setosa.jpg" width="300" />
    
    
    <figcaption>
        <h4>Iris setosa</h4>
        
    </figcaption>
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_virginica.jpg" width="300" />
    
    
    <figcaption>
        <h4>Iris virginica</h4>
        
    </figcaption>
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_versicolor.jpg" width="300" />
    
    
    <figcaption>
        <h4>Iris versicolor</h4>
        
    </figcaption>
    
</figure>
</p>

<p>The data is shown in a Jupyter (IPython) notebook in this <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/01_Iris_dataset_PCA.ipynb">repository</a>.</p>

<p>By converting the scikit-learn data into a <a href="http://pandas.pydata.org/">pandas</a> dataframe it can easily be
plotted using the <a href="http://stanford.edu/~mwaskom/software/seaborn/">seaborn</a> library.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/seaborn-iris-pairplot.png" width="800" />
    
    
    <figcaption>
        <h4>Seaborn iris plot</h4>
        
    </figcaption>
    
</figure>


<p>Using principal component analysis (PCA) the four dimensional data set can be
converted into a two dimensional data set by only choosing the first two
principal components.</p>


from sklearn.decomposition import PCA
pca = PCA(n_components=2)
iris_proj = pca.fit_transform(iris['data'])
print(iris['data'].shape)
print(iris_proj.shape)


<p>The first principal component explains 92.46% of the variance and the second
explains 5.30% of the variance.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/seaborn-iris-two-principal-components.png" width="800" />
    
    
    <figcaption>
        <h4>First two principal components of the Iris data</h4>
        
    </figcaption>
    
</figure>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/welcome/">
        Welcome
      </a>
    </h1>

    <span class="post-date">Sat, Feb 20, 2016</span>

    

<p>This web site is created using <a href="https://gohugo.io/">Hugo</a> a static web site generator.</p>

<p><img src="http://gavinln.github.io/blog-site/img/hugo/hugo-logo.png" width="200" height="226"/></p>

<h2 id="hyde-theme">Hyde Theme</h2>

<p>Hyde is an elegant open source and mobile first theme for Hugo. <a href="https://github.com/spf13/hyde">Hyde</a> a
two column theme that was ported from the theme of the same name made for
<a href="https://jekyllrb.com/">Jekyll</a> another static web site generator writen in <a href="https://www.ruby-lang.org/">Ruby</a>.</p>

<h2 id="making-a-post-using-hugo">Making a post using Hugo</h2>

<p>The content in Hugo is organized in sections. To make a new content file called
<code>welcome.html</code> in the section <code>post</code> run the following.</p>

<pre><code class="language-bash">hugo new post/welcome.md
</code></pre>

<h2 id="adding-images">Adding images</h2>

<p>To add an image to a markdown document you can use the following three options.</p>

<ol>
<li><p>Markdown syntax</p>

<pre><code>![Hugo](/img/hugo/hugo-logo_small.png)
</code></pre></li>

<li><p>HTML syntax</p>

<img src="http://gavinln.github.io/blog-site/img/hugo/hugo-logo.png" width="200" height="226"/></li>

<li><p>Hugo <a href="https://gohugo.io/extras/shortcodes/">shortcodes</a></p>

<p>{{&lt;figure src=&ldquo;/img/hugo/hugo-logo.png&rdquo; width=&ldquo;200&rdquo; height=&ldquo;226&rdquo;&gt;}}</p></li>
</ol>

  </div>
  
</div>
</div>

  </body>
</html>
