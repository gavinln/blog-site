<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.15" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Human Learning Machine Learning &middot; Human Learning Machine Learning </title>

  
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/poole.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/syntax.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://gavinln.github.io/blog-site/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="http://gavinln.github.io/blog-site/favicon.png">

  
  <link href="http://gavinln.github.io/blog-site/index.xml" rel="alternate" type="application/rss+xml" title="Human Learning Machine Learning" />
</head>

<body class="theme-base-0b">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://gavinln.github.io/blog-site/"><h1>Human Learning Machine Learning</h1></a>
      <p class="lead">
	Posts about Machine learning.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://gavinln.github.io/blog-site/">Home</a> </li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/"> Accuracy, precision, recall for machine learning </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/cross_validation/"> Cross validation </a></li>
      
      <li><a href="http://gavinln.github.io/blog-site/post">Index</a> </li>
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/cross_validation/">
        Cross validation
      </a>
    </h1>

    <span class="post-date">Thu, Apr 14, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">Cross-validation</a> is a technique used to assess how a statistical analysis will generalize to an independent data set.</p>

<p>When creating a predictive model, the model is trained using a dataset called the training dataset. The accuracy of the trained model is then tested on another unknown dataset called the testing dataset. The process is called cross-validation.</p>

<p><a href="http://scikit-learn.org/stable/modules/cross_validation.html">Scikit</a> learn makes it easy to use multiple methods for cross validation. A basic approach is called k-fold cross validation. The dataset is split into k smaller sets, where 1 of the sets is used to validate the model while the remaining are used to train the model. The <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">peformance measures</a> reported by the k-fold cross-validations are the average of the values computed by choosing a different set for the cross-validation and using the remaining for training.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/08_Cross_validation.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The &ldquo;Wisconsin Breast Cancer&rdquo; dataset is used to demonstrate cross-validation. This data set has 569 samples of which 357 are benign and 212 are malignant. Ten factors are used to predict breast cancer.</p>

<p>In addition to precision and recall, the <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> is calculated. The F1 score is the harmonic mean and equally weights precision and recall. A F1 score reaches its highest value at 1 and lowest value at 0.</p>

<p>The four classifiers: logistic regression, support vector, decision tree and random forests are compared on the cross-validation scores. They perform much worse on the test dataset as compared to the training dataset. Compare the results with those in the <a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/">previous post</a>.</p>

<table>
<thead>
<tr>
<th>classifier</th>
<th>accuracy</th>
<th>precision</th>
<th>recall</th>
<th>f1_score</th>
</tr>
</thead>

<tbody>
<tr>
<td>logistic regression</td>
<td>0.926186</td>
<td>0.938719</td>
<td>0.943978</td>
<td>0.941341</td>
</tr>

<tr>
<td>support vector (radial basis)</td>
<td>0.717047</td>
<td>0.704167</td>
<td>0.946779</td>
<td>0.807646</td>
</tr>

<tr>
<td>decision tree</td>
<td>0.905097</td>
<td>0.922006</td>
<td>0.927171</td>
<td>0.924581</td>
</tr>

<tr>
<td>random forest</td>
<td>0.947276</td>
<td>0.955432</td>
<td>0.960784</td>
<td>0.958101</td>
</tr>
</tbody>
</table>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/">
        Accuracy, precision, recall for machine learning
      </a>
    </h1>

    <span class="post-date">Sun, Apr 10, 2016</span>

    <p>A popular way to evaluate the performance of a machine learning algorithm is to use a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>. This is a table with two rows and two columns that displays the number of true positives, false positives, false negatives and true negatives.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/07_Precision_and_Recall.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The table below shows an example confusion matrix for a hypothetical test for a rare disease where only 2 people of out 100 have the disease. This is an unbalanced data set as a much larger number, 98 out of 100 do not have the disease. The first named row has cases of people who have the disease and the second named row has cases of people who do not have the disease. The first named column has people who test positive and the second named column has people who test negative.</p>

<p>This leads to four numeric cell with the top left containing true positive counts, the bottom left having false positive, the top right having false negative and the bottom right with true negative counts.</p>

<p>A simple way to create a very accurate test for this unbalanced example is to just assume everyone tests negative for the disease. This misses out on all the people who do actually have the disease and results in two false negative cases. However it correctly predicts 98 true negative cases. This results in a 98% accurate test. But this test cannot distinguish between people who have a disease and people who don&rsquo;t. Accuracy may not be a useful measure of the goodness of the test.</p>

<p>Two useful measures are precision and recall: Precision is a measure of how many of the selected items are relevant and recall is a measure of how many relevant items are selected.</p>

<p>precision = (true positives)/(true positives + false positives)</p>

<p>recall = (True positives)/positives</p>

<p>In the example below the precision is undefined while the recall is zero.</p>

<table>
<thead>
<tr>
<th></th>
<th align="right">predicted condition positive</th>
<th align="right">predicted condition negative</th>
</tr>
</thead>

<tbody>
<tr>
<td>true condition positive</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>

<tr>
<td>true condition negative</td>
<td align="right">0</td>
<td align="right">98</td>
</tr>
</tbody>
</table>

<p>An alternative test for the same rare disease where 2 out of 100 have the disease is show below. Now there is 1 true positive, 2 false positives, 1 false negative and 96 true negatives.</p>

<p>This test has a lower accuracy as it has correct predicted 97 out of 100 cases, lower than the previous test. This test also has a defined precision of 0.333333 and a recall of 0.5</p>

<p>This test correctly identifies 1 out of the 2 people who have the disease.</p>

<table>
<thead>
<tr>
<th></th>
<th align="right">predicted condition positive</th>
<th align="right">predicted condition negative</th>
</tr>
</thead>

<tbody>
<tr>
<td>true condition positive</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>

<tr>
<td>true condition negative</td>
<td align="right">2</td>
<td align="right">96</td>
</tr>
</tbody>
</table>

<p>To demonstrate the use of accuracy, precision and recall when measuring the peformance of a classifier, we use the &ldquo;Wisconsin Breast Cancer&rdquo; data set.</p>

<p>This data set has 569 samples of which 357 are benign and 212 are malignant</p>

<p>We predict whether the cancer is benign or malignant using ten factors: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and fractal dimension.</p>

<p>We compare four classifiers: logistic regression, support vector, decision tree and random forests on three different measures, accuracy, precision and recall. The decision tree and random forest classifiers are so good that they correctly classify 100% of the samples in this data set.</p>

<table>
<thead>
<tr>
<th>classifier</th>
<th align="right">accuracy</th>
<th align="right">precision</th>
<th align="right">recall</th>
</tr>
</thead>

<tbody>
<tr>
<td>logistic regression</td>
<td align="right">0.929701</td>
<td align="right">0.927224</td>
<td align="right">0.963585</td>
</tr>

<tr>
<td>support vector (radial basis)</td>
<td align="right">0.987698</td>
<td align="right">0.980769</td>
<td align="right">1</td>
</tr>

<tr>
<td>decision tree</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>

<tr>
<td>random forest</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>

<p>However the performance of a classifier on the training data is not necessarily
a good indication of how well it will do on data it has not seen. In future
posts the classifiers will be tested on new data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/k_nearest_neighbors/">
        K nearest neighbors using Scikit-learn
      </a>
    </h1>

    <span class="post-date">Sun, Mar 27, 2016</span>

    <p>The <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nearest neighbors</a> algorithm is one of the simplest algorithms for machine learning. It is a non-parametric method used for both classification and regression.</p>

<p>In a classification problem an object is classified by a majority vote of its neighbors. Typically k is a small positive integer. If k = 1, the object is assigned to be the class of the nearest neighbor. If k = 3 the object is assigned to be in the class of the nearest 2 neighbors and so on for different values of k.</p>

<p>In a regression problem, the property of the object is assigned a value that is the average of the values of its k nearest neighbors.</p>

<p>The <a href="http://scikit-learn.org/stable/modules/neighbors.html">Scikit-learn</a> library module KNeighborsClassifier demonstrates the use
of the k-nearest neighbor algorithm for classification.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/06_K_nearest_neighbors.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &ldquo;0&rdquo;, &ldquo;1&rdquo;, &ldquo;2&rdquo; (setosa, versicolor, virginica).</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The K-nearest neighbors classifier is used to predict the species by using just two features: &ldquo;sepal length&rdquo; and &ldquo;petal width&rdquo;.</p>

<p>The graphs below show the predictions of the k-nearest neighbors algorithm using three different values for the number of nearest neighbors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/k_nearest_neighbors/iris_multiple_values_k_nearest_neighbors.png" width="600" />
    
    
    <figcaption>
        <h4>Using k-nearest neighbors to predict Iris species</h4>
        
    </figcaption>
    
</figure>


<p>When the k value is small (like the graph on the left) the decision boundary is relatively complex and even though the algorithm predicts the training data well, it is likely over-fitting the data and fair poorly on a new sample. For a very high value of k (like the graph on the right) the method the decision boundary is simpler and likely to under-fit the training data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/scikit-random-forests/">
        Random forests using Scikit-learn
      </a>
    </h1>

    <span class="post-date">Sun, Mar 20, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Random_forest">Random forests</a> is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning</a> method. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained for any of the constituent learning algorithms.</p>

<p>Random forests work by constructing multiple decision trees and combining the trees. The algorithm was developed by <a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman</a> and Adele Cutler and &ldquo;Random Forests&rdquo; is their trademark.</p>

<p>Random forests correct for decision trees&rsquo; habit of over-fitting to their training data set.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/05_Random_Forests.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>To demonstrate the tendency of decision trees to overfit the data we predict the species of Iris using just two features: sepal length and petal width. The species are shown in a scatter plot in different colors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The graphs below show three Iris species using three different colors and the shaded regions predicted by the decision tree using lighter shades of the same colors. Each of the three plots in the set uses a different random sample made up of 70% of the data set. The decision tree boundaries are different in each case. This is an indication of over-fitting.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/random_forests/decision_trees-iris-multiple_subsets.png" width="600" />
    
    
    <figcaption>
        <h4>Using decision trees to predict Iris species</h4>
        
    </figcaption>
    
</figure>


<p>A similar plot shows a Random Forest Classifier with 500 trees each time used to select various sub-samples of the dataset. This controls over-fitting.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/random_forests/random_forests-iris-multiple_subsets.png" width="600" />
    
    
    <figcaption>
        <h4>Using Random Forests to predict Iris species</h4>
        
    </figcaption>
    
</figure>
</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://gavinln.github.io/blog-site/post/decision-trees-scikit/">
        Decision trees on the Iris data set
      </a>
    </h1>

    <span class="post-date">Sun, Mar 13, 2016</span>

    <p><a href="https://en.wikipedia.org/wiki/Decision_tree">Decision trees</a> are a non-parametric learning method used for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a>. Trees are often represented with a graph like model where each note is a test and each branch represents the outcome of the test.</p>

<p>We use the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a> to demonstrate the use of a decision tree classifier.</p>

<p>The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &ldquo;0&rdquo;, &ldquo;1&rdquo;, &ldquo;2&rdquo; (setosa, versicolor, virginica).</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/04_Decision_trees.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>To better display the performance of the decision trees algorithm we predict
the species of Iris using just two features: petal length and petal width. The
species are shown in a scatter plot in different colors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-petal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The output of the decision tree is shown using shaded regions that match the colors used to identify the flower. Using a decision tree with various depths the three species of Iris are classified, ineffectively at first with a tree of only one layer. As the number of layers increase the decision tree does a better job identifying the Iris species.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-multiple_depths.png" width="800" />
    
    
    <figcaption>
        <h4>Decision trees classification boundaries</h4>
        
    </figcaption>
    
</figure>


<p>The decision tree rules can also be represented using a graph like drawing with the root node on the left and the leaf nodes on the right.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-1.png" width="400" />
    
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-2.png" width="600" />
    
    
</figure>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-3.png" width="800" />
    
    
</figure>
</p>

<p>Finally we use a decision tree without limiting the depth. It classifies all the flowers correctly.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-unlimited_depth.png" width="600" />
    
    
</figure>
</p>

  </div>
  
</div>
</div>

  </body>
</html>
