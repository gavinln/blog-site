	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.16-DEV" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Support vector machines with Scikit learn &middot; Human Learning Machine Learning </title>

  
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/poole.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/syntax.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://gavinln.github.io/blog-site/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="http://gavinln.github.io/blog-site/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Human Learning Machine Learning" />
</head>

	<body class="theme-base-0b">
		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://gavinln.github.io/blog-site/"><h1>Human Learning Machine Learning</h1></a>
      <p class="lead">
         
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://gavinln.github.io/blog-site/">Home</a> </li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/accuracy-precision-recall/"> Accuracy, precision, recall for machine learning </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/cross_validation/"> Cross validation </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/gradient-boosting-classifier/"> Gradient boosting classifier </a></li>
      
        <li><a href="http://gavinln.github.io/blog-site/post/scaling-input-data/"> Scaling input data </a></li>
      
    </ul>

    <p>&copy; 2017. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>Support vector machines with Scikit learn</h1>
			  <span class="post-date">Sun, Mar 6, 2016</span>
			      <p><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support vector machines</a> are supervised learning models used for
classification and regression. For a classifier the data is represented as
points in space and a SVM classifier (SVC) separates the classes by a gap that
is as wide as possible. SVM algorithms are known as maximum margin classifiers.</p>

<p>To illustrate the SVC algorithm we generate random points in two dimensions
arranged in two clusters. This is illustrated in a Jupyter (IPython) notebook
in this <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/03_Support_vector_machines.ipynb">repository</a>.</p>

<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>X, y <span style="color: #666666">=</span> make_blobs(n_samples<span style="color: #666666">=50</span>, centers<span style="color: #666666">=2</span>, random_state<span style="color: #666666">=0</span>, cluster_std<span style="color: #666666">=0.60</span>)
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC
clf <span style="color: #666666">=</span> SVC(kernel<span style="color: #666666">=</span><span style="color: #BA2121">&#39;linear&#39;</span>)
clf<span style="color: #666666">.</span>fit(X, y)
</pre></div>



<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/linear_boundaries_two_clusters.png" width="500" />
    
    
    <figcaption>
        <h4>Two lines separating two clusters</h4>
        
    </figcaption>
    
</figure>


<p>Multiple lines can be drawn to separate the clusters. The black line is
preferred to the red line as there is a larger margin between it and the
nearest points.</p>

<p>Some of the points nearest the boundary are known as support vectors. They
margins and the support vectors are plotted below.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM_decision_function-margin-support_vectors.png" width="500" />
    
    
    <figcaption>
        <h4>Support vectors</h4>
        
    </figcaption>
    
</figure>
</p>

<p>Support vector classifiers are linear classifiers. For datasets that are not
linearly separable they do a poor job.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM-non_linearly_separable_data.png" width="500" />
    
    
    <figcaption>
        <h4>Non-linearly separable data</h4>
        
    </figcaption>
    
</figure>
</p>

<p>To create non-linear boundaries we could convert this two dimensional data set
to higher dimensions. For example we could add the distance of the points from
the origin as the third dimension. The two clusters will then be easily
separable.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/SVM-with_radial_basis_functions.png" width="500" />
    
    
    <figcaption>
        <h4>Radial basis functions for SVC</h4>
        
    </figcaption>
    
</figure>
</p>

<p>Another example with non-linearly separable data.</p>

<p>
<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/svm/radial_basis_functions-non_linearly_separable_data.png" width="500" />
    
    
    <figcaption>
        <h4>Radial basis functions for SVC</h4>
        
    </figcaption>
    
</figure>
</p>

			</div>

			
		</div>

  </body>
</html>
