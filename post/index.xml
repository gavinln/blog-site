<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Human Learning Machine Learning</title>
    <link>http://gavinln.github.io/blog-site/post/</link>
    <description>Recent content in Posts on Human Learning Machine Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 27 Mar 2016 23:17:09 -0700</lastBuildDate>
    <atom:link href="http://gavinln.github.io/blog-site/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>K nearest neighbors using Scikit-learn</title>
      <link>http://gavinln.github.io/blog-site/post/k_nearest_neighbors/</link>
      <pubDate>Sun, 27 Mar 2016 23:17:09 -0700</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/k_nearest_neighbors/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&#34;&gt;k-nearest neighbors&lt;/a&gt; algorithm is one of the simplest algorithms for machine learning. It is a non-parametric method used for both classification and regression.&lt;/p&gt;

&lt;p&gt;In a classification problem an object is classified by a majority vote of its neighbors. Typically k is a small positive integer. If k = 1, the object is assigned to be the class of the nearest neighbor. If k = 3 the object is assigned to be in the class of the nearest 2 neighbors and so on for different values of k.&lt;/p&gt;

&lt;p&gt;In a regression problem, the property of the object is assigned a value that is the average of the values of its k nearest neighbors.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://scikit-learn.org/stable/modules/neighbors.html&#34;&gt;Scikit-learn&lt;/a&gt; library module KNeighborsClassifier demonstrates the use
of the k-nearest neighbor algorithm for classification.&lt;/p&gt;

&lt;p&gt;This Jupyter &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/06_K_nearest_neighbors.ipynb&#34;&gt;notebook&lt;/a&gt; contains all the code used to plot the charts.&lt;/p&gt;

&lt;p&gt;The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &amp;ldquo;0&amp;rdquo;, &amp;ldquo;1&amp;rdquo;, &amp;ldquo;2&amp;rdquo; (setosa, versicolor, virginica).&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Scatter plot of Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The K-nearest neighbors classifier is used to predict the species by using just two features: &amp;ldquo;sepal length&amp;rdquo; and &amp;ldquo;petal width&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The graphs below show the predictions of the k-nearest neighbors algorithm using three different values for the number of nearest neighbors.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/k_nearest_neighbors/iris_multiple_values_k_nearest_neighbors.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Using k-nearest neighbors to predict Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;When the k value is small (like the graph on the left) the decision boundary is relatively complex and even though the algorithm predicts the training data well, it is likely over-fitting the data and fair poorly on a new sample. For a very high value of k (like the graph on the right) the method the decision boundary is simpler and likely to under-fit the training data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random forests using Scikit-learn</title>
      <link>http://gavinln.github.io/blog-site/post/scikit-random-forests/</link>
      <pubDate>Sun, 20 Mar 2016 22:18:21 -0700</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/scikit-random-forests/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random forests&lt;/a&gt; is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Ensemble_learning&#34;&gt;ensemble learning&lt;/a&gt; method. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained for any of the constituent learning algorithms.&lt;/p&gt;

&lt;p&gt;Random forests work by constructing multiple decision trees and combining the trees. The algorithm was developed by &lt;a href=&#34;https://en.wikipedia.org/wiki/Leo_Breiman&#34;&gt;Leo Breiman&lt;/a&gt; and Adele Cutler and &amp;ldquo;Random Forests&amp;rdquo; is their trademark.&lt;/p&gt;

&lt;p&gt;Random forests correct for decision trees&amp;rsquo; habit of over-fitting to their training data set.&lt;/p&gt;

&lt;p&gt;This Jupyter &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/05_Random_Forests.ipynb&#34;&gt;notebook&lt;/a&gt; contains all the code used to plot the charts.&lt;/p&gt;

&lt;p&gt;To demonstrate the tendency of decision trees to overfit the data we predict the species of Iris using just two features: sepal length and petal width. The species are shown in a scatter plot in different colors.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Scatter plot of Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The graphs below show three Iris species using three different colors and the shaded regions predicted by the decision tree using lighter shades of the same colors. Each of the three plots in the set uses a different random sample made up of 70% of the data set. The decision tree boundaries are different in each case. This is an indication of over-fitting.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/random_forests/decision_trees-iris-multiple_subsets.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Using decision trees to predict Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;A similar plot shows a Random Forest Classifier with 500 trees each time used to select various sub-samples of the dataset. This controls over-fitting.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/random_forests/random_forests-iris-multiple_subsets.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Using Random Forests to predict Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decision trees on the Iris data set</title>
      <link>http://gavinln.github.io/blog-site/post/decision-trees-scikit/</link>
      <pubDate>Sun, 13 Mar 2016 14:27:21 -0700</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/decision-trees-scikit/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree&#34;&gt;Decision trees&lt;/a&gt; are a non-parametric learning method used for &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_classification&#34;&gt;classification&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;regression&lt;/a&gt;. Trees are often represented with a graph like model where each note is a test and each branch represents the outcome of the test.&lt;/p&gt;

&lt;p&gt;We use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris data set&lt;/a&gt; to demonstrate the use of a decision tree classifier.&lt;/p&gt;

&lt;p&gt;The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &amp;ldquo;0&amp;rdquo;, &amp;ldquo;1&amp;rdquo;, &amp;ldquo;2&amp;rdquo; (setosa, versicolor, virginica).&lt;/p&gt;

&lt;p&gt;This Jupyter &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/04_Decision_trees.ipynb&#34;&gt;notebook&lt;/a&gt; contains all the code used to plot the charts.&lt;/p&gt;

&lt;p&gt;To better display the performance of the decision trees algorithm we predict
the species of Iris using just two features: petal length and petal width. The
species are shown in a scatter plot in different colors.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_species-petal_length-petal_width.png&#34; width=&#34;600&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Scatter plot of Iris species&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The output of the decision tree is shown using shaded regions that match the colors used to identify the flower. Using a decision tree with various depths the three species of Iris are classified, ineffectively at first with a tree of only one layer. As the number of layers increase the decision tree does a better job identifying the Iris species.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-multiple_depths.png&#34; width=&#34;800&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Decision trees classification boundaries&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The decision tree rules can also be represented using a graph like drawing with the root node on the left and the leaf nodes on the right.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-1.png&#34; width=&#34;400&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-2.png&#34; width=&#34;600&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/decision_trees/tree-depth-3.png&#34; width=&#34;800&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Finally we use a decision tree without limiting the depth. It classifies all the flowers correctly.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/decision_trees/decision_trees-unlimited_depth.png&#34; width=&#34;600&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Support vector machines with Scikit learn</title>
      <link>http://gavinln.github.io/blog-site/post/scikit-svm/</link>
      <pubDate>Sun, 06 Mar 2016 12:04:12 -0800</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/scikit-svm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Support_vector_machine&#34;&gt;Support vector machines&lt;/a&gt; are supervised learning models used for
classification and regression. For a classifier the data is represented as
points in space and a SVM classifier (SVC) separates the classes by a gap that
is as wide as possible. SVM algorithms are known as maximum margin classifiers.&lt;/p&gt;

&lt;p&gt;To illustrate the SVC algorithm we generate random points in two dimensions
arranged in two clusters. This is illustrated in a Jupyter (IPython) notebook
in this &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/03_Support_vector_machines.ipynb&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;X, y &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; make_blobs(n_samples&lt;span style=&#34;color: #666666&#34;&gt;=50&lt;/span&gt;, centers&lt;span style=&#34;color: #666666&#34;&gt;=2&lt;/span&gt;, random_state&lt;span style=&#34;color: #666666&#34;&gt;=0&lt;/span&gt;, cluster_std&lt;span style=&#34;color: #666666&#34;&gt;=0.60&lt;/span&gt;)
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;sklearn.svm&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; SVC
clf &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; SVC(kernel&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;)
clf&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;fit(X, y)
&lt;/pre&gt;&lt;/div&gt;



&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/svm/linear_boundaries_two_clusters.png&#34; width=&#34;500&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Two lines separating two clusters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Multiple lines can be drawn to separate the clusters. The black line is
preferred to the red line as there is a larger margin between it and the
nearest points.&lt;/p&gt;

&lt;p&gt;Some of the points nearest the boundary are known as support vectors. They
margins and the support vectors are plotted below.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/svm/SVM_decision_function-margin-support_vectors.png&#34; width=&#34;500&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Support vectors&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Support vector classifiers are linear classifiers. For datasets that are not
linearly separable they do a poor job.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/svm/SVM-non_linearly_separable_data.png&#34; width=&#34;500&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Non-linearly separable data&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;To create non-linear boundaries we could convert this two dimensional data set
to higher dimensions. For example we could add the distance of the points from
the origin as the third dimension. The two clusters will then be easily
separable.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/svm/SVM-with_radial_basis_functions.png&#34; width=&#34;500&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Radial basis functions for SVC&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Another example with non-linearly separable data.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/svm/radial_basis_functions-non_linearly_separable_data.png&#34; width=&#34;500&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Radial basis functions for SVC&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic regression on the Iris data set</title>
      <link>http://gavinln.github.io/blog-site/post/scikit-logistic_reg-iris/</link>
      <pubDate>Mon, 29 Feb 2016 21:30:18 -0800</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/scikit-logistic_reg-iris/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris data set&lt;/a&gt; has four features for Iris flower.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sepal length&lt;/li&gt;
&lt;li&gt;sepal width&lt;/li&gt;
&lt;li&gt;petal length&lt;/li&gt;
&lt;li&gt;petal width&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using a three class &lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html&#34;&gt;logistic regression&lt;/a&gt; the four features can be used to
classify the flowers into three species (Iris setosa, Iris virginica,
Iris versicolor).&lt;/p&gt;

&lt;p&gt;Using this Jupyter &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/02_Iris_dataset_logistic_regression.ipynb&#34;&gt;notebook&lt;/a&gt; combinations of two features we are used to
classify the species. The mis-predicted values are shown below.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;measure 1&lt;/th&gt;
&lt;th&gt;measure 2&lt;/th&gt;
&lt;th&gt;incorrect predictions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;sepal length&lt;/td&gt;
&lt;td&gt;sepal width&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sepal length&lt;/td&gt;
&lt;td&gt;petal length&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sepal length&lt;/td&gt;
&lt;td&gt;petal width&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sepal width&lt;/td&gt;
&lt;td&gt;petal length&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sepal width&lt;/td&gt;
&lt;td&gt;petal width&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;petal length&lt;/td&gt;
&lt;td&gt;petal width&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The previous &lt;a href=&#34;http://gavinln.github.io/blog-site/post/scikit-pca-iris/&#34;&gt;post&lt;/a&gt; shows that some combinations of
features are easier to use to separate the species than others.&lt;/p&gt;

&lt;p&gt;Logistic regression can also be used on the two principal components and
mis-predicts five specimens.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/seaborn-iris-two-principal-components-mis-predicted.png&#34; width=&#34;800&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Iris plot with mis-predicted items&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;A mesh when drawn over the plot shows the three classes of the logistic
regression.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/seaborn-iris-principal-components-logistic-reg-mesh.png&#34; width=&#34;800&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Iris plot - logistic regression&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PCA with Scikit learn on the Iris data set</title>
      <link>http://gavinln.github.io/blog-site/post/scikit-pca-iris/</link>
      <pubDate>Tue, 23 Feb 2016 19:11:35 -0800</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/scikit-pca-iris/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://scikit-learn.org/&#34;&gt;Scikit learn&lt;/a&gt; has multiple data sets included with the library. One of the most
well known data sets is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris data set&lt;/a&gt; introduced by Ronald Fisher.&lt;/p&gt;

&lt;p&gt;Four features were measured from each sample: the length and the width of the
sepals and petals, in centimetres. Sepals are usually green and typically
function as protection for the flower in bud, and often as support for the
petals when in bloom. Based on the combination of these four features the
goal is to distinguish between three species of Iris
(Iris setosa, Iris virginica and Iris versicolor).&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_setosa.jpg&#34; width=&#34;300&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Iris setosa&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_virginica.jpg&#34; width=&#34;300&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Iris virginica&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/iris_versicolor.jpg&#34; width=&#34;300&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Iris versicolor&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The data is shown in a Jupyter (IPython) notebook in this &lt;a href=&#34;https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/01_Iris_dataset_PCA.ipynb&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By converting the scikit-learn data into a &lt;a href=&#34;http://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt; dataframe it can easily be
plotted using the &lt;a href=&#34;http://stanford.edu/~mwaskom/software/seaborn/&#34;&gt;seaborn&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/seaborn-iris-pairplot.png&#34; width=&#34;800&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Seaborn iris plot&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Using principal component analysis (PCA) the four dimensional data set can be
converted into a two dimensional data set by only choosing the first two
principal components.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;sklearn.decomposition&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; PCA
pca &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; PCA(n_components&lt;span style=&#34;color: #666666&#34;&gt;=2&lt;/span&gt;)
iris_proj &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; pca&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;fit_transform(iris[&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;])
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;print&lt;/span&gt;(iris[&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;shape)
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;print&lt;/span&gt;(iris_proj&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;shape)
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;The first principal component explains 92.46% of the variance and the second
explains 5.30% of the variance.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://gavinln.github.io/blog-site/img/irises/seaborn-iris-two-principal-components.png&#34; width=&#34;800&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;First two principal components of the Iris data&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>http://gavinln.github.io/blog-site/post/welcome/</link>
      <pubDate>Sat, 20 Feb 2016 11:09:21 -0800</pubDate>
      
      <guid>http://gavinln.github.io/blog-site/post/welcome/</guid>
      <description>

&lt;p&gt;This web site is created using &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; a static web site generator.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://gavinln.github.io/blog-site/img/hugo/hugo-logo.png&#34; width=&#34;200&#34; height=&#34;226&#34;/&gt;&lt;/p&gt;

&lt;h2 id=&#34;hyde-theme:2cc7dc244eed4480e8b46c91e911e96b&#34;&gt;Hyde Theme&lt;/h2&gt;

&lt;p&gt;Hyde is an elegant open source and mobile first theme for Hugo. &lt;a href=&#34;https://github.com/spf13/hyde&#34;&gt;Hyde&lt;/a&gt; a
two column theme that was ported from the theme of the same name made for
&lt;a href=&#34;https://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt; another static web site generator writen in &lt;a href=&#34;https://www.ruby-lang.org/&#34;&gt;Ruby&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;making-a-post-using-hugo:2cc7dc244eed4480e8b46c91e911e96b&#34;&gt;Making a post using Hugo&lt;/h2&gt;

&lt;p&gt;The content in Hugo is organized in sections. To make a new content file called
&lt;code&gt;welcome.html&lt;/code&gt; in the section &lt;code&gt;post&lt;/code&gt; run the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo new post/welcome.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;adding-images:2cc7dc244eed4480e8b46c91e911e96b&#34;&gt;Adding images&lt;/h2&gt;

&lt;p&gt;To add an image to a markdown document you can use the following three options.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Markdown syntax&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![Hugo](/img/hugo/hugo-logo_small.png)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;HTML syntax&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;img&lt;/span&gt; &lt;span style=&#34;color: #7D9029&#34;&gt;src&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;/img/hugo/hugo-logo.png&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #7D9029&#34;&gt;width&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;200&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #7D9029&#34;&gt;height&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;226&amp;quot;&lt;/span&gt;/&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hugo &lt;a href=&#34;https://gohugo.io/extras/shortcodes/&#34;&gt;shortcodes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;{{&amp;lt;figure src=&amp;ldquo;/img/hugo/hugo-logo.png&amp;rdquo; width=&amp;ldquo;200&amp;rdquo; height=&amp;ldquo;226&amp;rdquo;&amp;gt;}}&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>