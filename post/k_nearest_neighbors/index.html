	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.15" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> K nearest neighbors using Scikit-learn &middot; Human Learning Machine Learning </title>

  
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/poole.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/syntax.css">
  <link rel="stylesheet" href="http://gavinln.github.io/blog-site/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://gavinln.github.io/blog-site/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="http://gavinln.github.io/blog-site/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Human Learning Machine Learning" />
</head>

	<body class="theme-base-0b">
		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://gavinln.github.io/blog-site/"><h1>Human Learning Machine Learning</h1></a>
      <p class="lead">
	Posts about Machine learning.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://gavinln.github.io/blog-site/">Home</a> </li>
      
      <li><a href="http://gavinln.github.io/blog-site/post">Index</a> </li>
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>K nearest neighbors using Scikit-learn</h1>
			  <span class="post-date">Sun, Mar 27, 2016</span>
			      <p>The <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nearest neighbors</a> algorithm is one of the simplest algorithms for machine learning. It is a non-parametric method used for both classification and regression.</p>

<p>In a classification problem an object is classified by a majority vote of its neighbors. Typically k is a small positive integer. If k = 1, the object is assigned to be the class of the nearest neighbor. If k = 3 the object is assigned to be in the class of the nearest 2 neighbors and so on for different values of k.</p>

<p>In a regression problem, the property of the object is assigned a value that is the average of the values of its k nearest neighbors.</p>

<p>The <a href="http://scikit-learn.org/stable/modules/neighbors.html">Scikit-learn</a> library module KNeighborsClassifier demonstrates the use
of the k-nearest neighbor algorithm for classification.</p>

<p>This Jupyter <a href="https://github.com/gavinln/stats_py_vm/blob/master/notebooks/scikit-learn/06_K_nearest_neighbors.ipynb">notebook</a> contains all the code used to plot the charts.</p>

<p>The Iris data set has four features (sepal length, sepal width, petal length, petal width) which can be used to classify Iris flowers into three species denoted as &ldquo;0&rdquo;, &ldquo;1&rdquo;, &ldquo;2&rdquo; (setosa, versicolor, virginica).</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/irises/iris_species-sepal_length-petal_width.png" width="600" />
    
    
    <figcaption>
        <h4>Scatter plot of Iris species</h4>
        
    </figcaption>
    
</figure>


<p>The K-nearest neighbors classifier is used to predict the species by using just two features: &ldquo;sepal length&rdquo; and &ldquo;petal width&rdquo;.</p>

<p>The graphs below show the predictions of the k-nearest neighbors algorithm using three different values for the number of nearest neighbors.</p>


<figure >
    
        <img src="http://gavinln.github.io/blog-site/img/k_nearest_neighbors/iris_multiple_values_k_nearest_neighbors.png" width="600" />
    
    
    <figcaption>
        <h4>Using k-nearest neighbors to predict Iris species</h4>
        
    </figcaption>
    
</figure>


<p>When the k value is small (like the graph on the left) the decision boundary is relatively complex and even though the algorithm predicts the training data well, it is likely over-fitting the data and fair poorly on a new sample. For a very high value of k (like the graph on the right) the method the decision boundary is simpler and likely to under-fit the training data.</p>

			</div>

			
		</div>

  </body>
</html>
